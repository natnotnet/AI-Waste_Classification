{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g-jvdD7rtP_v"
   },
   "source": [
    "# EfficientNet Version 6 (RGB)\n",
    "This notebook trains the 6th version of the EfficientNet model, which modifies version 5 to train on the RGB dataset (with the second iteration of pre-processing)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mFY3CS45tc_Y"
   },
   "source": [
    "## 1. Mount Google Drive (Colab Only)\n",
    "This section connects to Google Drive to access the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18001,
     "status": "ok",
     "timestamp": 1745225629669,
     "user": {
      "displayName": "nat ang",
      "userId": "01830563404866135370"
     },
     "user_tz": -480
    },
    "id": "y_CALYcKtIvM",
    "outputId": "d62378b2-42bf-4d2a-fec4-628327179204"
   },
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "DATASET_PATH = '/content/drive/My Drive/50.021 Artificial Intelligence Group Assignment'\n",
    "\n",
    "%cd \"/content/drive/My Drive/50.021 Artificial Intelligence Group Assignment\"\n",
    "\n",
    "import os\n",
    "assert os.path.exists(DATASET_PATH), \"[!] Dataset path does not exist. Please check the path.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2F1J7FC_ti72"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "def is_running_in_colab():\n",
    "    try:\n",
    "        import google.colab\n",
    "        return True\n",
    "    except ImportError:\n",
    "        return False\n",
    "\n",
    "if is_running_in_colab():\n",
    "  %pip install lightning polars\n",
    "  %pip install icecream rich tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wRJfWUtBthOP"
   },
   "source": [
    "## 2. Import Libraries\n",
    "Imports all required packages including PyTorch, Lightning, and data processing tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QtfEPzqbtmtS"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import lightning as L\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "from datetime import datetime\n",
    "\n",
    "# Model imports\n",
    "from torchvision.models import (\n",
    "    efficientnet_v2_s, EfficientNet_V2_S_Weights,\n",
    "    efficientnet_v2_m, EfficientNet_V2_M_Weights,\n",
    "    efficientnet_v2_l, EfficientNet_V2_L_Weights\n",
    ")\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "# Metrics imports\n",
    "from torchmetrics import (\n",
    "    Accuracy,\n",
    "    F1Score,\n",
    "    AUROC,\n",
    "    ConfusionMatrix,\n",
    "    Precision,\n",
    "    Recall\n",
    ")\n",
    "\n",
    "# Lightning modules and callbacks\n",
    "from lightning.pytorch.callbacks import (\n",
    "    ModelCheckpoint,\n",
    "    EarlyStopping,\n",
    "    LearningRateMonitor\n",
    ")\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1745225794975,
     "user": {
      "displayName": "nat ang",
      "userId": "01830563404866135370"
     },
     "user_tz": -480
    },
    "id": "AswyutdDtmqZ",
    "outputId": "12a59847-ab00-4683-b4a2-01e8de6b6143"
   },
   "outputs": [],
   "source": [
    "print(\"GPU available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9BqM7u8ZtrQp"
   },
   "source": [
    "## 3. Define Dataset Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ruVXdsLRtmny"
   },
   "outputs": [],
   "source": [
    "class ImageDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Dataset for loading and preprocessing garbage classification images.\"\"\"\n",
    "\n",
    "    def __init__(self, dataframe: pl.DataFrame, training=False, img_size=224):\n",
    "        super().__init__()\n",
    "        # Use the image_path column directly from the dataframe as it now contains full paths\n",
    "        # to the preprocessed RGB images\n",
    "        self.image_path = dataframe['image_path'].to_numpy().squeeze()\n",
    "        self.garbage_type = dataframe.select('label').to_numpy().squeeze()\n",
    "        self.garbage_to_idx = {garbage: i for i, garbage in enumerate(np.unique(self.garbage_type))}\n",
    "        self.training = training\n",
    "        self.img_size = img_size\n",
    "\n",
    "        # Enhanced augmentation for training - can be simpler now since images are already preprocessed\n",
    "        self.train_transforms = v2.Compose([\n",
    "            v2.RandomHorizontalFlip(p=0.5),\n",
    "            v2.RandomVerticalFlip(p=0.3),\n",
    "            v2.RandomRotation(degrees=10),\n",
    "            # Reduced augmentation as images are already preprocessed and augmented\n",
    "        ])\n",
    "\n",
    "        # Base transforms for all images\n",
    "        self.transforms = v2.Compose([\n",
    "            v2.ToDtype(torch.float32, scale=True),\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_path)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_path[idx]\n",
    "\n",
    "        try:\n",
    "            # Load the image - now these are RGB processed images\n",
    "            image = cv2.imread(image_path)\n",
    "            if image is None:\n",
    "                raise ValueError(f\"Failed to load image at {image_path}\")\n",
    "\n",
    "            # Convert BGR to RGB (cv2 loads as BGR)\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            # Resize to the target size\n",
    "            image = cv2.resize(image, (self.img_size, self.img_size))\n",
    "\n",
    "            # Convert to tensor and reorder dimensions to [C, H, W]\n",
    "            image = torch.from_numpy(image).float().permute(2, 0, 1)\n",
    "\n",
    "            # Normalize to [0, 1]\n",
    "            image = image / 255.0\n",
    "\n",
    "            # Apply augmentations if in training mode\n",
    "            if self.training:\n",
    "                image = self.train_transforms(image)\n",
    "\n",
    "            # Apply basic transforms\n",
    "            image = self.transforms(image)\n",
    "\n",
    "            # Get class label\n",
    "            garbage = self.garbage_to_idx[self.garbage_type[idx]]\n",
    "\n",
    "            return image, torch.tensor(garbage, dtype=torch.long)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {image_path}: {e}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "18NMkwOAuRWH"
   },
   "outputs": [],
   "source": [
    "class GarbageClassificationData(L.LightningDataModule):\n",
    "    \"\"\"Data module for garbage classification dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, ws_root: Path = Path(\".\"), batch_size=32, num_workers=0, img_size=224):\n",
    "        super().__init__()\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.img_size = img_size\n",
    "\n",
    "        # Updated paths to use the new RGB preprocessed images\n",
    "        rgb_base_path = Path(\"/content/drive/My Drive/50.021 Artificial Intelligence Group Assignment/RGB_preprocessed_images\")\n",
    "\n",
    "        # Load datasets using the new CSV files\n",
    "        self.train_ds = ImageDataset(\n",
    "            pl.read_csv(rgb_base_path / 'train.csv'),\n",
    "            training=True,\n",
    "            img_size=self.img_size\n",
    "        )\n",
    "        self.val_ds = ImageDataset(\n",
    "            pl.read_csv(rgb_base_path / 'validation.csv'),\n",
    "            img_size=self.img_size\n",
    "        )\n",
    "        self.test_ds = ImageDataset(\n",
    "            pl.read_csv(rgb_base_path / 'test.csv'),\n",
    "            img_size=self.img_size\n",
    "        )\n",
    "\n",
    "        # Get class info\n",
    "        self.n_classes = len(self.train_ds.garbage_to_idx)\n",
    "        self.idx_to_garbage = {v: k for k, v in self.train_ds.garbage_to_idx.items()}\n",
    "\n",
    "        # Print dataset statistics\n",
    "        print(f\"Number of training samples: {len(self.train_ds)}\")\n",
    "        print(f\"Number of validation samples: {len(self.val_ds)}\")\n",
    "        print(f\"Number of test samples: {len(self.test_ds)}\")\n",
    "        print(f\"Number of classes: {self.n_classes}\")\n",
    "        print(f\"Classes: {self.idx_to_garbage}\")\n",
    "\n",
    "        # DataLoader settings\n",
    "        self.dataloader_extras = dict(\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=True,\n",
    "            persistent_workers=num_workers > 0\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.train_ds,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            **self.dataloader_extras\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.val_ds,\n",
    "            batch_size=self.batch_size*2,\n",
    "            **self.dataloader_extras\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.test_ds,\n",
    "            batch_size=self.batch_size*2,\n",
    "            **self.dataloader_extras\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tjS9jMzJw_R3"
   },
   "source": [
    "## 4. Base Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6AzAVlq9uazm"
   },
   "outputs": [],
   "source": [
    "class BaseGarbageClassifier(L.LightningModule):\n",
    "    \"\"\"Base class for all garbage classification models.\"\"\"\n",
    "\n",
    "    def __init__(self, n_classes, learning_rate=1e-3):\n",
    "        super().__init__()\n",
    "        self.n_classes = n_classes\n",
    "        self.learning_rate = learning_rate\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        # Initialize metrics\n",
    "        self.train_acc = Accuracy(task=\"multiclass\", num_classes=n_classes)\n",
    "        self.val_acc = Accuracy(task=\"multiclass\", num_classes=n_classes)\n",
    "\n",
    "        # Test metrics\n",
    "        self.test_accuracy = Accuracy(task=\"multiclass\", num_classes=n_classes)\n",
    "        self.test_precision = Precision(task=\"multiclass\", num_classes=n_classes, average='macro')\n",
    "        self.test_recall = Recall(task=\"multiclass\", num_classes=n_classes, average='macro')\n",
    "        self.test_f1 = F1Score(task=\"multiclass\", num_classes=n_classes, average='macro')\n",
    "        self.test_auroc = AUROC(task=\"multiclass\", num_classes=n_classes)\n",
    "        self.test_confusion_matrix = ConfusionMatrix(task=\"multiclass\", num_classes=n_classes)\n",
    "\n",
    "        # Save hyperparameters for checkpointing\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass of the model. To be implemented by subclasses.\"\"\"\n",
    "        raise NotImplementedError(\"Subclasses must implement forward()\")\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_pred = self(x)\n",
    "        loss = self.criterion(y_pred, y)\n",
    "\n",
    "        # Calculate and log metrics\n",
    "        preds = torch.argmax(y_pred, dim=1)\n",
    "        acc = self.train_acc(preds, y)\n",
    "\n",
    "        # Log metrics\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        self.log('train_acc', acc, on_step=True, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_pred = self(x)\n",
    "        loss = self.criterion(y_pred, y)\n",
    "\n",
    "        # Calculate metrics\n",
    "        preds = torch.argmax(y_pred, dim=1)\n",
    "        acc = self.val_acc(preds, y)\n",
    "\n",
    "        # Log metrics\n",
    "        self.log('val_loss', loss, on_epoch=True, prog_bar=True)\n",
    "        self.log('val_acc', acc, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_pred = self(x)\n",
    "        loss = self.criterion(y_pred, y)\n",
    "\n",
    "        # Calculate predictions\n",
    "        preds = torch.argmax(y_pred, dim=1)\n",
    "        probs = torch.softmax(y_pred, dim=1)\n",
    "\n",
    "        # Update metrics\n",
    "        self.test_accuracy(preds, y)\n",
    "        self.test_precision(preds, y)\n",
    "        self.test_recall(preds, y)\n",
    "        self.test_f1(preds, y)\n",
    "        self.test_auroc(probs, y)\n",
    "        self.test_confusion_matrix(preds, y)\n",
    "\n",
    "        # Log metrics\n",
    "        self.log('test_loss', loss, on_epoch=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        # Compute and log final metrics\n",
    "        accuracy = self.test_accuracy.compute()\n",
    "        precision = self.test_precision.compute()\n",
    "        recall = self.test_recall.compute()\n",
    "        f1_score = self.test_f1.compute()\n",
    "        auroc = self.test_auroc.compute()\n",
    "        conf_mat = self.test_confusion_matrix.compute().cpu().numpy()\n",
    "\n",
    "        # Log metrics\n",
    "        self.log('test_accuracy', accuracy)\n",
    "        self.log('test_precision', precision)\n",
    "        self.log('test_recall', recall)\n",
    "        self.log('test_f1', f1_score)\n",
    "        self.log('test_auroc', auroc)\n",
    "\n",
    "        # Print detailed metrics\n",
    "        print(\"\\n--- Test Metrics ---\")\n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"Precision: {precision:.4f}\")\n",
    "        print(f\"Recall: {recall:.4f}\")\n",
    "        print(f\"F1 Score: {f1_score:.4f}\")\n",
    "        print(f\"AUROC: {auroc:.4f}\")\n",
    "\n",
    "        # Visualize Confusion Matrix\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues')\n",
    "        plt.title('Test Confusion Matrix')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"confusion_matrix_{self.__class__.__name__}.png\")\n",
    "        plt.show()\n",
    "\n",
    "        # Reset metrics\n",
    "        self.test_accuracy.reset()\n",
    "        self.test_precision.reset()\n",
    "        self.test_recall.reset()\n",
    "        self.test_f1.reset()\n",
    "        self.test_auroc.reset()\n",
    "        self.test_confusion_matrix.reset()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"Configure the optimizer and learning rate scheduler.\"\"\"\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            self.parameters(),\n",
    "            lr=self.learning_rate,\n",
    "            weight_decay=1e-4\n",
    "        )\n",
    "\n",
    "        # Learning rate scheduler\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer,\n",
    "            mode='min',\n",
    "            factor=0.5,\n",
    "            patience=2,\n",
    "            min_lr=1e-6\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"monitor\": \"val_loss\",\n",
    "                \"interval\": \"epoch\",\n",
    "                \"frequency\": 1\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uABEx4SLuo7x"
   },
   "source": [
    "## 5. Training Utilities / Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LO4lUXyXusGF"
   },
   "outputs": [],
   "source": [
    "def train_and_evaluate_model(model_class, data_module, model_name, max_epochs=10, learning_rate=1e-3):\n",
    "    \"\"\"Train and evaluate a model without checkpointing.\"\"\"\n",
    "\n",
    "    # Create timestamp for run identification\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    run_name = f\"{model_name}_{timestamp}\"\n",
    "\n",
    "    # Create logger (for metrics, not checkpoints)\n",
    "    logger = TensorBoardLogger(\n",
    "        save_dir='logs',\n",
    "        name=run_name,\n",
    "        log_graph=False  # Disable graph logging to reduce overhead\n",
    "    )\n",
    "\n",
    "    # Create callbacks (no checkpoint callback)\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=5,\n",
    "        mode='min'\n",
    "    )\n",
    "\n",
    "    lr_monitor = LearningRateMonitor(logging_interval='epoch')\n",
    "\n",
    "    # Initialize trainer without checkpoint callback\n",
    "    trainer = L.Trainer(\n",
    "        max_epochs=max_epochs,\n",
    "        accelerator='gpu' if torch.cuda.is_available() else 'cpu',\n",
    "        precision=\"16-mixed\" if torch.cuda.is_available() else \"32-true\",\n",
    "        callbacks=[early_stopping, lr_monitor],\n",
    "        logger=logger,\n",
    "        log_every_n_steps=10,\n",
    "        enable_checkpointing=False  # Disable checkpointing\n",
    "    )\n",
    "\n",
    "    # Initialize model\n",
    "    model = model_class(n_classes=data_module.n_classes, learning_rate=learning_rate)\n",
    "\n",
    "    # Train the model\n",
    "    print(f\"\\nTraining {model_name}...\")\n",
    "    trainer.fit(model=model, datamodule=data_module)\n",
    "\n",
    "    # Test the model\n",
    "    print(f\"\\nEvaluating {model_name}...\")\n",
    "    trainer.test(model=model, datamodule=data_module)\n",
    "\n",
    "    # Save the final model (after training and testing)\n",
    "    try:\n",
    "        model_save_path = f\"{model_name}_{timestamp}_final.pt\"\n",
    "        # Save only the model state dict, not the entire checkpoint\n",
    "        torch.save(model.state_dict(), model_save_path)\n",
    "        print(f\"Model saved to {model_save_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not save model due to: {e}\")\n",
    "        print(\"Continuing without saving...\")\n",
    "\n",
    "    # Return the trained model and its metrics\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UCrCc85HU27Z"
   },
   "source": [
    "## Method 6: Modifying Method 5 to train on RGB Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MPmFPmo8VI4z"
   },
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import OneCycleLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6cGlGahCU9Tc"
   },
   "outputs": [],
   "source": [
    "class EfficientNetV2MOptimized(L.LightningModule):\n",
    "    \"\"\"\n",
    "    Enhanced EfficientNetV2-M with Optimized Regularization:\n",
    "    1. Learning rate scheduling\n",
    "    2. Progressive unfreezing\n",
    "    3. Advanced regularization techniques\n",
    "    4. Mixup data augmentation\n",
    "    5. Optimized training process\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_classes, learning_rate=3e-4, weight_decay=1e-4):\n",
    "        super().__init__()\n",
    "        self.n_classes = n_classes\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "        self.mixup_alpha = 0.2  # Parameter for mixup augmentation\n",
    "\n",
    "        # Load pre-trained EfficientNetV2-M model\n",
    "        self.model = efficientnet_v2_m(weights=EfficientNet_V2_M_Weights.DEFAULT)\n",
    "\n",
    "        # IMPORTANT: No need to modify the first conv layer for RGB inputs\n",
    "        # since the model already expects 3 channels by default\n",
    "        # REMOVE these lines as they were converting the model to accept grayscale\n",
    "        # original_conv = self.model.features[0][0]\n",
    "        # new_conv = nn.Conv2d(...)\n",
    "        # self.model.features[0][0] = new_conv\n",
    "\n",
    "        # Initialize frozen layer flags for progressive unfreezing\n",
    "        self.unfreeze_stage = 0\n",
    "\n",
    "        # Initially freeze all feature extraction layers\n",
    "        for i in range(len(self.model.features)):\n",
    "            for param in self.model.features[i].parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        # Get the number of features in the final layer\n",
    "        in_features = self.model.classifier[1].in_features\n",
    "\n",
    "        # Replace classifier with enhanced MLP head\n",
    "        self.model.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.4),  # Increased dropout rate\n",
    "            nn.Linear(in_features, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.SiLU(),  # SiLU/Swish activation (better than ReLU)\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, n_classes)\n",
    "        )\n",
    "\n",
    "        # Use label smoothing cross entropy\n",
    "        self.criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "\n",
    "        # Initialize metrics\n",
    "        self.train_acc = Accuracy(task=\"multiclass\", num_classes=n_classes)\n",
    "        self.val_acc = Accuracy(task=\"multiclass\", num_classes=n_classes)\n",
    "\n",
    "        # Test metrics\n",
    "        self.test_accuracy = Accuracy(task=\"multiclass\", num_classes=n_classes)\n",
    "        self.test_precision = Precision(task=\"multiclass\", num_classes=n_classes, average='macro')\n",
    "        self.test_recall = Recall(task=\"multiclass\", num_classes=n_classes, average='macro')\n",
    "        self.test_f1 = F1Score(task=\"multiclass\", num_classes=n_classes, average='macro')\n",
    "        self.test_auroc = AUROC(task=\"multiclass\", num_classes=n_classes)\n",
    "        self.test_confusion_matrix = ConfusionMatrix(task=\"multiclass\", num_classes=n_classes)\n",
    "\n",
    "        # Save hyperparameters\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def mixup_data(self, x, y):\n",
    "        \"\"\"Apply mixup augmentation to the batch.\"\"\"\n",
    "        if self.training and self.mixup_alpha > 0:\n",
    "            lam = np.random.beta(self.mixup_alpha, self.mixup_alpha)\n",
    "            batch_size = x.size()[0]\n",
    "            index = torch.randperm(batch_size).to(x.device)\n",
    "\n",
    "            mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "            y_a, y_b = y, y[index]\n",
    "            return mixed_x, y_a, y_b, lam\n",
    "        else:\n",
    "            return x, y, y, 1.0\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "\n",
    "        # Apply mixup if training\n",
    "        if self.training:\n",
    "            x, y_a, y_b, lam = self.mixup_data(x, y)\n",
    "            y_pred = self(x)\n",
    "            loss = lam * self.criterion(y_pred, y_a) + (1 - lam) * self.criterion(y_pred, y_b)\n",
    "        else:\n",
    "            y_pred = self(x)\n",
    "            loss = self.criterion(y_pred, y)\n",
    "\n",
    "        # Calculate and log metrics\n",
    "        preds = torch.argmax(y_pred, dim=1)\n",
    "        acc = self.train_acc(preds, y)\n",
    "\n",
    "        # Log metrics\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        self.log('train_acc', acc, on_step=True, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_pred = self(x)\n",
    "        loss = self.criterion(y_pred, y)\n",
    "\n",
    "        # Calculate metrics\n",
    "        preds = torch.argmax(y_pred, dim=1)\n",
    "        acc = self.val_acc(preds, y)\n",
    "\n",
    "        # Log metrics\n",
    "        self.log('val_loss', loss, on_epoch=True, prog_bar=True)\n",
    "        self.log('val_acc', acc, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_pred = self(x)\n",
    "        loss = self.criterion(y_pred, y)\n",
    "\n",
    "        # Calculate predictions\n",
    "        preds = torch.argmax(y_pred, dim=1)\n",
    "        probs = torch.softmax(y_pred, dim=1)\n",
    "\n",
    "        # Update metrics\n",
    "        self.test_accuracy(preds, y)\n",
    "        self.test_precision(preds, y)\n",
    "        self.test_recall(preds, y)\n",
    "        self.test_f1(preds, y)\n",
    "        self.test_auroc(probs, y)\n",
    "        self.test_confusion_matrix(preds, y)\n",
    "\n",
    "        # Log metrics\n",
    "        self.log('test_loss', loss, on_epoch=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        # Compute and log final metrics\n",
    "        accuracy = self.test_accuracy.compute()\n",
    "        precision = self.test_precision.compute()\n",
    "        recall = self.test_recall.compute()\n",
    "        f1_score = self.test_f1.compute()\n",
    "        auroc = self.test_auroc.compute()\n",
    "        conf_mat = self.test_confusion_matrix.compute().cpu().numpy()\n",
    "\n",
    "        # Store metrics in self for access after training\n",
    "        self.final_metrics = {\n",
    "            'accuracy': accuracy.item(),\n",
    "            'precision': precision.item(),\n",
    "            'recall': recall.item(),\n",
    "            'f1': f1_score.item(),\n",
    "            'auroc': auroc.item()\n",
    "        }\n",
    "\n",
    "        # Print detailed metrics\n",
    "        print(\"\\n--- Test Metrics ---\")\n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"Precision: {precision:.4f}\")\n",
    "        print(f\"Recall: {recall:.4f}\")\n",
    "        print(f\"F1 Score: {f1_score:.4f}\")\n",
    "        print(f\"AUROC: {auroc:.4f}\")\n",
    "\n",
    "        # Reset metrics\n",
    "        self.test_accuracy.reset()\n",
    "        self.test_precision.reset()\n",
    "        self.test_recall.reset()\n",
    "        self.test_f1.reset()\n",
    "        self.test_auroc.reset()\n",
    "        self.test_confusion_matrix.reset()\n",
    "\n",
    "    def on_train_epoch_start(self):\n",
    "        \"\"\"\n",
    "        Progressive unfreezing of layers based on training epoch.\n",
    "        \"\"\"\n",
    "        # Unfreeze more layers as training progresses\n",
    "        total_blocks = len(self.model.features)\n",
    "\n",
    "        if self.current_epoch == 3 and self.unfreeze_stage == 0:\n",
    "            # Unfreeze the last 30% of feature layers after 3 epochs\n",
    "            unfreeze_from = int(total_blocks * 0.7)\n",
    "            for i in range(unfreeze_from, total_blocks):\n",
    "                for param in self.model.features[i].parameters():\n",
    "                    param.requires_grad = True\n",
    "            print(f\"Unfreezing layers from {unfreeze_from} to {total_blocks-1}\")\n",
    "            self.unfreeze_stage = 1\n",
    "\n",
    "        elif self.current_epoch == 6 and self.unfreeze_stage == 1:\n",
    "            # Unfreeze more layers after 6 epochs\n",
    "            unfreeze_from = int(total_blocks * 0.4)\n",
    "            for i in range(unfreeze_from, total_blocks):\n",
    "                for param in self.model.features[i].parameters():\n",
    "                    param.requires_grad = True\n",
    "            print(f\"Unfreezing layers from {unfreeze_from} to {total_blocks-1}\")\n",
    "            self.unfreeze_stage = 2\n",
    "\n",
    "        elif self.current_epoch == 9 and self.unfreeze_stage == 2:\n",
    "            # Unfreeze all layers after 9 epochs\n",
    "            for i in range(total_blocks):\n",
    "                for param in self.model.features[i].parameters():\n",
    "                    param.requires_grad = True\n",
    "            print(\"Unfreezing all layers\")\n",
    "            self.unfreeze_stage = 3\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"\n",
    "        Configure optimizer with weight decay and learning rate scheduler.\n",
    "        \"\"\"\n",
    "        # Create optimizer with weight decay\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            self.parameters(),\n",
    "            lr=self.learning_rate,\n",
    "            weight_decay=self.weight_decay\n",
    "        )\n",
    "\n",
    "        # OneCycleLR scheduler\n",
    "        scheduler = {\n",
    "            \"scheduler\": OneCycleLR(\n",
    "                optimizer,\n",
    "                max_lr=self.learning_rate,\n",
    "                total_steps=self.trainer.estimated_stepping_batches,\n",
    "                pct_start=0.1,  # 10% warmup\n",
    "                div_factor=25,  # initial_lr = max_lr/25\n",
    "                final_div_factor=1000,  # min_lr = initial_lr/1000\n",
    "            ),\n",
    "            \"interval\": \"step\",\n",
    "            \"frequency\": 1\n",
    "        }\n",
    "\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bXfD5YW1vo5_"
   },
   "source": [
    "## Training + Evaluation of methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 693,
     "status": "ok",
     "timestamp": 1745225796017,
     "user": {
      "displayName": "nat ang",
      "userId": "01830563404866135370"
     },
     "user_tz": -480
    },
    "id": "hxzttf7EVhXx",
    "outputId": "d993035a-adad-4068-bde2-85d4969c6a5a"
   },
   "outputs": [],
   "source": [
    "# Prepare data module (shared between models)\n",
    "data_module = GarbageClassificationData(\n",
    "    batch_size=16,\n",
    "    num_workers=4,\n",
    "    img_size=224\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n_Nhn9JtWd91"
   },
   "source": [
    "### Method 6 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "e8e54ed548cc4fc3bb36b84cd73ab17d",
      "f1ce6d45ecc84c6c9864c86363e57429",
      "ff7dff09a60a46cd82f370413129c241",
      "3951274401b3419c964a71a7e5bfd9f2",
      "d8b1797d408d4dfba156e75815c96921",
      "9f075392b798408b9a5f25029dfb901b",
      "c69e3e67ea1a435fbc92039a44422262",
      "60f422d4edd24ab9abd6529bf6fe23de",
      "19c93c778c06462085815b802be1c4ed",
      "af2f2508ccbb4be982496002c8d4a86c",
      "929da64eb7ca4722a041349c6422349b",
      "6c4b71280b754798bc187d1cecdd9ad4",
      "fb8eac73cbe44587a5231f6bf0c84504",
      "725afbb0bcb445479a78428e6ff2cdb8",
      "84a263e7cc364196a85a400df67ac2ca",
      "28fe757e32444910b41195262c9c19d0",
      "2d14d3d27d9d43eeb1539eda9838eef7",
      "51fccfe202474e278eb90b2c93e58b3e",
      "f4ec481daea94bec9a0b3db15e15ba97",
      "6f13fbef0e9b454fa53370e1e740c9e8",
      "72714d5d5b8543bd962f58ee3c7e33b9",
      "b805a9b2488a4c46a5cbf3c90e611af2",
      "e5f343e9152c4482b40e9527f2932a06",
      "7015d633c5684d249b87f679f4ec59c4",
      "62e627f0be5945419bd1417d4d9218fa",
      "e86c70869ab248b99424c2a5e16e4c2e",
      "b0d3e826a40d41fc9f597865ae6f2f89",
      "1a50a32966904fa9baea2f26a9dede28",
      "302772101851422783d59bec2024f831",
      "ac3cf40bfd3c4560aa9223865f9c14c8",
      "82682fd0f6534ba892426f9c3480dc76",
      "3bdcb68cbccf4890ad94da46147cabc2",
      "2d7b93e779c44a6e801272863d076e03",
      "e6a88045dd4644458225f3b4d3b85c7b",
      "531037f6c7cd49bc9466c5e72904b551",
      "b8456ac660ce4a97802d04e67fef7144",
      "7990b7acc8894c23b46d5fe83094c374",
      "e59d88d063934b0eaa1e932363159d70",
      "ffd3a7daf2b940219c2ec890b2835bc0",
      "a2cb9fb9e7e64c5b8e9e1e3df306ab54",
      "f7a4111b2baf4343bca6635175c27059",
      "1036d9176f3d4441b05dbebb57fede7d",
      "ba9880018ab449b9b843cfe24f2cad38",
      "14c186ca3df14b519bc3053062638049",
      "5cf6381219cf48dfab5f4cdca0d169d7",
      "3587912babb047b2981ff085251b32e0",
      "dd8b60bb575145b6b1da0089105abc53",
      "d6b35137a3d04b6599f84a7275cc44a5",
      "ca9c0c1f7c424150b16ad3383c638aa1",
      "852b54b193034dedb667b2ae5129b8c8",
      "aeea3a0376a746548d20c8f932bbc695",
      "32b7e437e48f4c9aa7432428127dde22",
      "86c48ad76ab84ad994c33b1e57a3f82e",
      "ad0b01ad052e467e9df371eacaf21e9c",
      "f778c5b9d5df4dd1b66c05352419ecb0",
      "fd52a1bf46f04eb6b9999c2a4fcda4dc",
      "d8d2a688d24749fab1c4e26d23d70166",
      "a58bc93e0fe14a88b949fc4bd2a2ffbe",
      "b25608e9b8b34ff59ca143627252c88e",
      "43f1fc8cfc054b50bea9c2f299bf96b8",
      "9a5e86d5ff79484fb717752b7aea2cde",
      "923957ae2aca4e5685386d4ac3549ac7",
      "14fe1923a76c4fa6a6deec15f7cc8f9a",
      "f637fbc18eab4228b8c454c12b52f67b",
      "83fecbd6a02a45d088bc132eaf6958a5",
      "d527a764a2154ea0a1b59c0b5435d95b",
      "3573e7ab6dae4de28abfc7bd42081f8d",
      "4e8b7cab2b1a4ecaa3767106cc832e7c",
      "9aee13ffb6b14d32a84f31328fddc013",
      "406956797a3c4bdd83a439a577d34aae",
      "83ac5b2bf8fe4434ae5e66441ffff6c6",
      "21f1b89284cc48c585a8ce3d197771aa",
      "1982b5f23d174c65b97974f4d5aebb65",
      "b089935bdc2444d8968d714c6fde43ae",
      "eb2eddeac61043b9902d05a0c413c5dc",
      "988bacf264aa498dbc5d8c2aa763ecff",
      "e541555eeb664e7d9969804660794746",
      "bba755ac2534461cb643a4d35e8d0892",
      "39d2df708a23489d9c5c77054b8e5833",
      "3c033f6d9c214c44b86435c2f18c2362",
      "669e32782a16443e8d3cce2ca54ad0af",
      "727cd1c53cc348dbb552e13a66dd4885",
      "293b46cc44594ede920b2bee2b95623f",
      "7c30ea0baa744316a769182f30d7e041",
      "d4d2097f704341b198e76388584465a8",
      "cfd380c530da47c0a4328042c3580080",
      "6f11307af5f540209996b0d59e9cde08",
      "0ffb6e9cda97465c82e2ce286d471df0",
      "e0762834d11444f59524179938fca26c",
      "47a52f5041bb4f4baa9018e55c892ab7",
      "f075e036fc0d47239fd63dc35d5902b1",
      "470c69157d9940d2affa48622739cce1",
      "25897e94ce9a423d85846c55e699d4f4",
      "b5cf32cc044f42eaa9433345edf19578",
      "2101682e3cbf451ba4738f8ac9ab3356",
      "88dd023febdf4166bd0a8b00a3978ef5",
      "c42a4393ead246a89119b168232fa806",
      "3c9d1cdbebfe4c0396f1b1007dc976a2",
      "b231e9cc0a7148e8a3ab5a6e2088dfc5",
      "1903a793697b45e8a5628a43331d7533",
      "302a46e989a844e49a12625dcc373ebd",
      "489b9f68ee6c4c50aabe1c9020b3dba9",
      "a6184150bd904df1873e44ab6f81c25f",
      "96182c97d85a427e8f6ae5afca4323cf",
      "5cff7372fc5842f0879a32fdee8f1433",
      "c58a6181d0254bce909d1cb9252702e5",
      "b6b55bd45bff4e0eb9762c0eadcb547b",
      "91dd90e9a08d4e3da036e677efee3dc5",
      "20a60f647d684f68a640b1a60cca79e4",
      "ee97332ff9f84ec187a35281ee916c33",
      "097ca1432a2642d0a95bcada7d98b08b",
      "4b31f09d493640aa83f77f445c5d9c3b",
      "94e27014ff76431e94cb87da3cd2c74c",
      "5eec17392c1a494f98963bd69250d138",
      "5a4fa2fbdf6d494da0b7f837bf3b09f7",
      "32cd92e78c2d49c38f690a72ee630649",
      "218f933d46a24e3383b6ef716c90da42",
      "0755042537364da095de4ecd40fbc716",
      "f142fe22bfdc4ac091fcdec31c1041d4",
      "8ae19ebad41244fc992707393b2537b2",
      "92bd7f10155d47c89636d961483ceb4f",
      "262ed82d488843c998709cc920f6af05",
      "6253aa69620c48cd8ef7b843bef068d9",
      "f43e33dffa8244e7a72793bdc716d06f",
      "97a1d60f057641afa8a0d1e2ff99f128",
      "66c9548fca4d409d960f65a136f464f6",
      "6519d35b85ac44bcbfc675a60eb6c706",
      "704026e7411445d3bfef33e1a1318b4e",
      "ec919bf09ffd4003ab762074d9b6b128",
      "ba56758de1ea49468dc2c37a172341f4",
      "c21f1b6e52244c659fbb5ff902a7d2f4",
      "6d4a6de93a4a457abf95a1d4b58960d2",
      "73364116a9c64d069fec10edebfe08ab",
      "1382842ba0d14ae684b40f404c401198",
      "7f2262d89e71490792846b72d1b115a0",
      "19483e40745445e29e846496835802b8",
      "8b07350cb9f14bc2a82d890835e40e34",
      "0bb7b237ad8048cea570e8e03a6e62b1",
      "3450cca194354d4981e54b4a1663d27f",
      "376ced0342394112bff08dc844549cd3",
      "3c16fb136d61460e95a34f017f62d2f0",
      "b4848c6d88394954a89ce212d1e9c961",
      "3e3a798257f04bc786520ba93fb6f0df",
      "22a84a1ae31248b5859171e1302d98b4",
      "6bc6f81b96064f2c898b7e051ba50acf",
      "09df4c68fe7e45dcb788fd6f9180f2c9",
      "02dc94a690c9460787e749c595890a3a",
      "51dcc3d064af412eb478eca5d1a943f2",
      "68df9e84e06d406a8029492117dadf77",
      "8e0d00e513a940d7a8e0609d7e79ac9c",
      "2a6cd29d16ce48b89bbb06310a91d300",
      "142871a4b9634c1b9e1822b360940d71",
      "18461221c0794a0a871e040572f89a45",
      "c366944e70cb4525822f295d475ed844"
     ]
    },
    "executionInfo": {
     "elapsed": 3243330,
     "status": "ok",
     "timestamp": 1744877965110,
     "user": {
      "displayName": "capstone nat",
      "userId": "06830734093637736730"
     },
     "user_tz": -480
    },
    "id": "9VdHTt7eWfCL",
    "outputId": "2cb5d5c6-218a-44e4-e7e2-9bcbc46c026f"
   },
   "outputs": [],
   "source": [
    "# Model 5\n",
    "print(\"\\n=== Training Model 5: Enhanced EfficientNetV2-M with Optimized Regularization ===\")\n",
    "model5 = train_and_evaluate_model(\n",
    "  model_name=\"EfficientNetV2MOptimized\",\n",
    "  model_class=EfficientNetV2MOptimized,\n",
    "  data_module=data_module,\n",
    "  max_epochs=20,\n",
    "  learning_rate=3e-4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-AV3Pn6KWiOh"
   },
   "outputs": [],
   "source": [
    "torch.save(model5, \"saved_efficientnet/efficientnetv2-method5-RGB.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n4z6YztDX6Gx"
   },
   "source": [
    "## Load saved model and test with images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 44,
     "status": "ok",
     "timestamp": 1745226215715,
     "user": {
      "displayName": "nat ang",
      "userId": "01830563404866135370"
     },
     "user_tz": -480
    },
    "id": "CKyFonr0iHUs",
    "outputId": "ba8027f8-9781-45c5-fce9-79ddcdaceb02"
   },
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5668,
     "status": "ok",
     "timestamp": 1745226284166,
     "user": {
      "displayName": "nat ang",
      "userId": "01830563404866135370"
     },
     "user_tz": -480
    },
    "id": "eFfyQtm6k2fH",
    "outputId": "38c53cf1-a190-4bbf-ae56-bc90cb1a9856"
   },
   "outputs": [],
   "source": [
    "model = torch.load(\"saved_efficientnet/efficientnetv2-method5-RGB.pth\", map_location=device, weights_only=False)\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 128,
     "status": "ok",
     "timestamp": 1745226404335,
     "user": {
      "displayName": "nat ang",
      "userId": "01830563404866135370"
     },
     "user_tz": -480
    },
    "id": "YhZmCIibmXkb",
    "outputId": "e1e71fd1-48a7-49db-fdda-6ce1b95d73e9"
   },
   "outputs": [],
   "source": [
    "data_module = GarbageClassificationData()\n",
    "test_dataset = data_module.test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "output_embedded_package_id": "1m4nPa-rFqBmrh5hr-K19p1jk6AgMUZQr"
    },
    "executionInfo": {
     "elapsed": 13556,
     "status": "ok",
     "timestamp": 1745226628619,
     "user": {
      "displayName": "nat ang",
      "userId": "01830563404866135370"
     },
     "user_tz": -480
    },
    "id": "Ks0EMvuEj9nd",
    "outputId": "b28145b8-7fc3-4116-b00e-6041c238f1f6"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Pick random test samples\n",
    "num_samples = 20\n",
    "indices = random.sample(range(len(test_dataset)), num_samples)\n",
    "\n",
    "for i in indices:\n",
    "    image, label = test_dataset[i]\n",
    "    image_input = image.unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(image_input)\n",
    "        pred = torch.argmax(output, dim=1).item()\n",
    "\n",
    "    image_np = image.permute(1, 2, 0).cpu().numpy()\n",
    "\n",
    "    plt.imshow(image_np)\n",
    "    plt.title(f\"True: {data_module.idx_to_garbage[label.item()]}, Predicted: {data_module.idx_to_garbage[pred]}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IZUVDuvVj9lF"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nz7Udz7myH6x"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
